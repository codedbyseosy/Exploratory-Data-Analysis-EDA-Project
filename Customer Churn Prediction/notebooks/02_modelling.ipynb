{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10dd639",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The following objective will be accomplished in this notebook:\n",
    "\n",
    "1. Leveraging techniques such as preprocessing, feature engineering, selection, and transformation to build robust models that can identify customers at risk of leaving the company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105aa8e3",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc7468a",
   "metadata": {},
   "source": [
    "### 1.1. Importing the libraries and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc32738",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel '.venv (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "file_path = '../data/telco_customer_churn.csv' # Define the relative path to the dataset\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # If the file exists, read it into a pandas DataFrame\n",
    "    dataset_uncleaned = pd.read_csv(file_path)\n",
    "else:\n",
    "    # If the file does not exist, raise an error with a helpful message\n",
    "    raise FileNotFoundError(f\"The file was not found at {file_path}\")\n",
    "\n",
    "# Display the first 10 rows \n",
    "dataset_uncleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d35f0",
   "metadata": {},
   "source": [
    "### 1.2. Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e63596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'TotalCharges' column to numeric values\n",
    "dataset_uncleaned['TotalCharges'] = pd.to_numeric(dataset_uncleaned['TotalCharges'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows that contain NaN values\n",
    "dataset = dataset_uncleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa1842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d348dc",
   "metadata": {},
   "source": [
    "Dataset is now clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243aee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm shape of dataset\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962fd75",
   "metadata": {},
   "source": [
    "### 1.3. Feature engineering\n",
    "\n",
    "This section aims to improve the accuracy and performance of the machine learning models by transforming the unrefined data into features with higher predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e5778e",
   "metadata": {},
   "source": [
    "#### 1.3.1. Reducing redundancy and long labelling for improved readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c1d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and values to replace\n",
    "cols_to_edit = {\n",
    "    'MultipleLines': 'No phone service',\n",
    "    ('OnlineSecurity',  'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'): 'No internet service'\n",
    "}\n",
    "\n",
    "\n",
    "# Replace \"No phone/internet service\" with \"No\"\n",
    "for key, value in cols_to_edit.items():\n",
    "    if isinstance(key, tuple):\n",
    "        for col in key:\n",
    "            dataset[f\"{col}_categorised\"] = dataset[col].replace(value, 'No')\n",
    "    else:\n",
    "        dataset[f\"{key}_categorised\"] = dataset[key].replace(value, 'No')\n",
    "\n",
    "\n",
    "# # Simplify PaymentMethod values\n",
    "# dataset['PaymentMethod'] = dataset['PaymentMethod'].replace({\n",
    "#     'Electronic check': 'Automatic',\n",
    "#     'Mailed check': 'Manual',\n",
    "#     'Bank transfer (automatic)': 'Automatic',\n",
    "#     'Credit card (automatic)': 'Automatic',\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dba5ed2",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding these features:</u>**\n",
    "\n",
    "Phone and internet-related services were collasped into two categories.\n",
    "- This was done to avoid VIF scores of infinity which indicates high-levels of multicollinearity and in turn will affect linear-based models.\n",
    "- Original columns will be kept for tree and gradient boosting models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ec57c",
   "metadata": {},
   "source": [
    "#### 1.3.2. Addressing skewness in 'MonthlyCharges' and 'TotalCharges'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8518f1",
   "metadata": {},
   "source": [
    "Because 'TotalCharges' is heavily right-skewed and 'MonthlyCharges' is slightly left-skewed, log1p will be applied to both columns to transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3534364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transform MonthlyCharges\n",
    "dataset['MonthlyCharges_log'] = np.log1p(dataset['MonthlyCharges'])\n",
    "\n",
    "# Log transform TotalCharges\n",
    "dataset['TotalCharges_log'] = np.log1p(dataset['TotalCharges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc0143",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding these features:</u>**\n",
    "\n",
    "Although tree-based models and gradient boosting models don’t care about skewness because they split based on thresholds and not distributions, this transformation will help with linear models such as (LogisticRegression, etc.) which are sensitive to highly skewed variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcc7f9",
   "metadata": {},
   "source": [
    "#### 1.3.3. Binning tenure into categories for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3358d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the custom bins for categorising customer tenure (in months)\n",
    "bins = [0, 24, 48, 73]\n",
    "\n",
    "# Defining the corresponding labels for each bin\n",
    "labels = ['Short term', 'Mid term', 'Long term']\n",
    "\n",
    "# Categorising 'tenure' values into bins \n",
    "dataset['tenure_bin'] = pd.cut(dataset['tenure'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227cea0",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding this feature:</u>**\n",
    "\n",
    "Models such as LogisticRegression are sometimes better at identifying key patterns in categorised or binned feautures than raw numerical inputs such as tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61da481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pie chart to show the distribution of customers by tenure bin\n",
    "dataset['tenure_bin'].value_counts().plot(kind='pie', autopct='%1.1f%%', \n",
    "                                          startangle=90, colors=['#FF5C8D','#FFB6C1', '#F8C8DC'], \n",
    "                                          wedgeprops={'edgecolor': 'black', 'linewidth': 1})\n",
    "\n",
    "# Add title\n",
    "plt.title('Customer Distribution by Tenure Bin')\n",
    "\n",
    "plt.ylabel('')  # Remove y-label\n",
    "\n",
    "# Display chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b926cb",
   "metadata": {},
   "source": [
    "**<u>Observation:</u>** This corroborates the observations in the EDA notebook. There are more short-term and long-term customers than mid-term customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41126974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom color palette\n",
    "custom_palette = ['#FF5C8D', '#F8C8DC']  # soft pink \n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Create count plot showing the churn distribution by tenure bin\n",
    "sns.countplot(x='tenure_bin', hue='Churn', data=dataset, palette=custom_palette, edgecolor='black', linewidth=1, saturation=1)\n",
    "\n",
    "# Add title\n",
    "plt.title('Churn Distribution by Tenure Bin')\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel('Tenure Group')\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c290fb",
   "metadata": {},
   "source": [
    "**<u>Observation:</u>** Corroborates the results observed during the EDA stage. Short-term customers have much higher churn rates than any other tenure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d9df1",
   "metadata": {},
   "source": [
    "**<u>Notes</u>**\n",
    "- Keep both tenure and tenure_bin to see if both features improve the results in the tree and gradient boosting models.\n",
    "    - This is to give the models both raw precision and a more interpretable abstraction.\n",
    "- Drop tenure and keep tenure_bin when using models that don’t handle raw numeric data well or that perform better with simplified inputs (like logistic regression). \n",
    "    - This is also to reduce multicollinearity or complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2399f058",
   "metadata": {},
   "source": [
    "#### 1.3.4. Binning MonthlyCharges into pricing tiers\n",
    "\n",
    "Dividing the 'MonthlyCharges' variable into tiers based on prices (fixed ranges) for interpretation and business insight.\n",
    "\n",
    "- Basic -> $18 - $40 (low spenders, maybe budget-conscious)\n",
    "\n",
    "- Standard -> $40 - $70 (avergae users, mid-range services)\n",
    "\n",
    "- Premium -> $70 - $95 (heavier users, possibly multiple services)\n",
    "\n",
    "- Platinum -> $95 - $120 (top-tier spenders, very engaged customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962aae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to classify customers into pricing tiers based on MonthlyCharges\n",
    "def monthly_plans(plan):\n",
    "    # If monthly charge is less than $40 -> Basic plan\n",
    "    if plan < 40: \n",
    "        return 'Basic'\n",
    "    # If monthly charge is between $40 and $70 -> Standard plan\n",
    "    elif plan < 70:\n",
    "        return 'Standard'\n",
    "    # If monthly charge is between $70 and $95 -> Premium plan\n",
    "    elif plan < 95:\n",
    "        return 'Premium'\n",
    "    # If $95 or more -> Platinum plan\n",
    "    else:\n",
    "        return 'Platinum'\n",
    "\n",
    "# Apply the function to create a new column 'monthly_pricing_tiers'\n",
    "dataset['monthly_pricing_tiers'] = dataset['MonthlyCharges'].apply(monthly_plans)\n",
    "\n",
    "# Count the number of customers in each pricing tier\n",
    "dataset['monthly_pricing_tiers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92cdc8",
   "metadata": {},
   "source": [
    "**<u>Observation:</u>** Most customers are on premium plans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9829c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pie chart to show the distribution of customers across monthly pricing tiers\n",
    "dataset['monthly_pricing_tiers'].value_counts().plot(\n",
    "    kind='pie',\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=['#FF5C8D','#FFB6C1', '#ffcccb', '#F8C8DC'],  # pinks \n",
    "    wedgeprops={'edgecolor': 'black', 'linewidth': 1}  # adds borders\n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title('Customer Distribution by Monthly Pricing Tier')\n",
    "\n",
    "plt.ylabel('')  # Remove y-label\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64dbf1",
   "metadata": {},
   "source": [
    "**<u>Observation:</u>** Aligns with the value count above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Create the count plot to show churn distribution across pricing tiers\n",
    "sns.countplot(\n",
    "    x='monthly_pricing_tiers',\n",
    "    hue='Churn',\n",
    "    data=dataset,\n",
    "    order=['Basic', 'Standard', 'Premium', 'Platinum'],\n",
    "    palette=['#FF5C8D', '#F8C8DC'],  # custom colors\n",
    "    edgecolor='black',\n",
    "    linewidth=1,\n",
    "    saturation=1 \n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title('Churn Distribution by Monthly Pricing Tier', fontsize=14, weight='bold')\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel('Pricing Tier', fontsize=12)\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ba280",
   "metadata": {},
   "source": [
    "**<u>Observation:</u>** Results still hold true with the results in the EDA. Customers paying around $71-$95 dollars are the most likely to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d95a11",
   "metadata": {},
   "source": [
    "#### 1.3.5. How far off are the reported TotalCharges from what we’d expect (MonthlyCharges * tenure)?\n",
    "\n",
    "- Positive charge_diff -> Customers have paid more than expected (overpaid)\n",
    "- Negative charge_diff -> Customers have paid less than expected (maybe due to waived fees, discounts, or churn interruptions) (underpaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a1acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column to check for discrepancies\n",
    "dataset['charge_diff'] = dataset['TotalCharges'] - (dataset['MonthlyCharges'] * dataset['tenure'])\n",
    "\n",
    "# Display column to inspect discrepancies\n",
    "dataset[['charge_diff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad086379",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding this feature:</u>**\n",
    "\n",
    "- Column could highlight billing issues, discounts, or partial months\n",
    "- Column could be a proxy for irregularity in a customer's payment history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1439df13",
   "metadata": {},
   "source": [
    "#### 1.3.6 Assessing the discrepancies found in charge_diff\n",
    "\n",
    "- Billing issue: If the difference is large (positive or negative).\n",
    "Example: If |charge_diff| is greater than 1 month's worth of charges -> that's suspicious.\n",
    "\n",
    "- Discounts: If |charge_diff| is consistently negative but not huge.\n",
    "Example: A customer has paid less than expected -> maybe because of promotions/discounts.\n",
    "\n",
    "- Partial months: If |charge_diff| is small (positive or negative), often less than 1 month's charge.\n",
    "Example: If someone joined mid-month or left mid-month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c89ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to categorise potential billing issues for each customer\n",
    "def billing_issue(row):\n",
    "\n",
    "    charge_diff = row['charge_diff']\n",
    "    monthly_charge = row['MonthlyCharges']\n",
    "\n",
    "    # If a customer joined/left mid-month\n",
    "    if abs(charge_diff) < (0.5 * monthly_charge):\n",
    "        return 'partial_month'\n",
    "    \n",
    "    # If a customer consistently paid less\n",
    "    elif  (-1 * monthly_charge) <= charge_diff <= (-0.5 * monthly_charge):\n",
    "        return 'discount'\n",
    "    \n",
    "    # If something is a billing issue\n",
    "    elif abs(charge_diff) > monthly_charge:\n",
    "        return 'billing_issue'\n",
    "    \n",
    "    # Perfectly aligned\n",
    "    elif charge_diff == 0:\n",
    "        return 'ok'\n",
    "    \n",
    "    return 'ok' # default match\n",
    "\n",
    "# Apply row-wise\n",
    "dataset['billing_flag'] = dataset.apply(billing_issue, axis = 1)\n",
    "dataset['billing_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d3d3b",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "\n",
    "- partial_month (3,499 customers, ~50%)\n",
    "    - Very common. Likely customers who joined or left mid-cycle. This makes sense as the dataset has lots of recent joiners and churners.\n",
    "    - Interpretation: Not necessarily a problem, but shows how many “non-standard” months exist.\n",
    "\n",
    "- billing_issue (1,947 customers, ~28%)\n",
    "    - This is large. Since threshold is the “absolute difference > 1 month’s charge,” then there are serious mismatches between the expected and actual payments.\n",
    "    - Interpretation: This group is worth investigating — it could be due to data quality issues, unusual billing practices, or misapplied fees.\n",
    "\n",
    "- discount (820 customers, ~12%)\n",
    "    - These customers consistently pay less than their expected monthly rate.\n",
    "    - Interpretation: Customers could be on promotional offers, loyalty discounts, or bundled packages. Valuable for retention insight (discounted customers may churn less).\n",
    "\n",
    "- ok (766 customers, ~11%)\n",
    "    - Everything matches perfectly — “clean” billing.\n",
    "    - Interpretation: This is the baseline group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd84b8d",
   "metadata": {},
   "source": [
    "##### 1.3.6.1. Further analyising the billing _flag column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a704a",
   "metadata": {},
   "source": [
    "##### 1.3.6.1.1. (billing_flag vs Churn) \n",
    "\n",
    "Analysing whether billing issues, discounts, etc. are related to customer churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96084c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[['billing_flag', 'Churn']]\n",
    "\n",
    "# Create a normalised cross-tabulation (contingency table) between billing_flag and churn\n",
    "cross_tab_billing_flag_with_churn = pd.crosstab(dataset['billing_flag'], dataset['Churn'], normalize='index') * 100\n",
    "\n",
    "# Print the percentage table\n",
    "print(cross_tab_billing_flag_with_churn.round(2))\n",
    "\n",
    "# Print a separator line for readability\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec60e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of billing_flag vs churn as percentages\n",
    "cross_tab_billing_flag_with_churn = pd.crosstab(\n",
    "    dataset['billing_flag'], \n",
    "    dataset['Churn'], \n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "# Define custom colour palette\n",
    "colors = ['#FF5C8D', '#F8C8DC']  # pinks\n",
    "\n",
    "# Plot the crosstab as a stacked bar plot\n",
    "ax = cross_tab_billing_flag_with_churn.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=colors,\n",
    "    figsize=(8, 6),\n",
    "    edgecolor='black',   \n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Style adjustments for the plot\n",
    "plt.title(\"Churn Distribution by Billing Flag\", fontsize=16, weight='bold', color='#333')\n",
    "plt.xlabel(\"Billing Flag\", fontsize=13, labelpad=10)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=13, labelpad=10)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "# Add % labels on each stacked bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.1f%%\", label_type=\"center\", color=\"black\", fontsize=11, weight=\"bold\")\n",
    "\n",
    "# Customise legend appearance\n",
    "plt.legend(title=\"Churn\", fontsize=11, title_fontsize=12, loc=\"upper right\", frameon=False)\n",
    "\n",
    "# Remove frame for cleaner look\n",
    "plt.box(False)\n",
    "\n",
    "# Adjust spacing for a neat layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8323f4",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "- Customers flagged as partial_month churn the most by far (≈38%).\n",
    "    - Likely because they are joining/leaving mid-cycle, so they’re more transient.\n",
    "\n",
    "- Discount and ok groups have similar churn (~18%).\n",
    "\n",
    "- Billing issues surprisingly churn less (13%). This could mean:\n",
    "    - They’re locked into longer contracts, or\n",
    "    - Some billing discrepancies aren’t actually frustrating enough to trigger churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a618dc",
   "metadata": {},
   "source": [
    "##### 1.3.6.1.2. (billing_flag vs Contract) \n",
    "\n",
    "This will identify which groups experience the most billing anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846beee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a normalised cross-tabulation (contingency table) between billing_flag and Contract\n",
    "cross_tab_billing_flag_with_contract = pd.crosstab(dataset['billing_flag'], dataset['Contract'], normalize='index') * 100\n",
    "\n",
    "# Print the percentage table\n",
    "print(cross_tab_billing_flag_with_contract.round(2))\n",
    "\n",
    "# Print a separator line for readability\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab of billing_flag vs Contract as percentages\n",
    "cross_tab_billing_flag_with_contract = pd.crosstab(\n",
    "    dataset['billing_flag'], \n",
    "    dataset['Contract'], \n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "print(cross_tab_billing_flag_with_contract.round(2))\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Define custom colour palette\n",
    "colors = ['#FF5C8D','#FFB6C1', '#F8C8DC']  # pink\n",
    "\n",
    "# Plot the crosstab as a stacked bar plot\n",
    "ax = cross_tab_billing_flag_with_contract.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    color=colors,\n",
    "    figsize=(8, 6),\n",
    "    edgecolor='black',\n",
    "    linewidth=1.2,\n",
    "     alpha=1\n",
    ")\n",
    "\n",
    "# Style adjustments for the plot\n",
    "plt.title(\"Contract Distribution by Billing Flag\", fontsize=16, weight='bold', color='#333')\n",
    "plt.xlabel(\"Billing Flag\", fontsize=13, labelpad=10)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=13, labelpad=10)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=11)\n",
    "\n",
    "# Add % labels on each stacked bar\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt=\"%.1f%%\", label_type=\"center\", color=\"black\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "# Customise legend appearance\n",
    "plt.legend(title=\"Contract Type\", fontsize=11, title_fontsize=12, loc=\"lower right\", frameon=False)\n",
    "\n",
    "# Remove frame for cleaner look\n",
    "plt.box(False)\n",
    "\n",
    "# Adjust spacing for a neat layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e97adc",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "- Partial_month overwhelmingly happens in Month-to-month plans (71%).\n",
    "    - Makes sense: people start/stop often. These are the riskiest churn customers.\n",
    "\n",
    "- Billing issues are skewed to Two-year contracts (40%).\n",
    "    - Suggests long-term customers may encounter billing mismatches more often. They don’t churn quickly, but they may become dissatisfied silently.\n",
    "\n",
    "- Discounts are distributed, but lean toward shorter contracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9c8b3",
   "metadata": {},
   "source": [
    "##### 1.3.6.1.3. (billing_flag vs monthly_pricing_tiers) \n",
    "\n",
    "Analysing billing issues the relationship between billing discrepanices and monthly_pricing_tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae4a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a normalised cross-tabulation (contingency table) between billing_flag and monthly_pricing_tiers\n",
    "cross_tab_billing_flag_with_pricing_tiers = pd.crosstab(dataset['billing_flag'], dataset['monthly_pricing_tiers'], normalize='index') * 100\n",
    "\n",
    "# Print the percentage table\n",
    "print(cross_tab_billing_flag_with_pricing_tiers.round(2))\n",
    "\n",
    "# Print a separator line for readability\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a712251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# Create a crosstab of billing_flag vs monthly_pricing_tiers as percentages\n",
    "cross_tab_billing_flag_with_pricing_tiers = pd.crosstab(\n",
    "    dataset['billing_flag'],\n",
    "    dataset['monthly_pricing_tiers'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "# Enforce a column order for the tiers\n",
    "order = ['Basic', 'Standard', 'Premium', 'Platinum']\n",
    "cols = [c for c in order if c in cross_tab_billing_flag_with_pricing_tiers.columns]\n",
    "ct = cross_tab_billing_flag_with_pricing_tiers[cols] if cols else cross_tab_billing_flag_with_pricing_tiers\n",
    "\n",
    "# Define custom colour palette\n",
    "colors = ['#FF5C8D','#FFB6C1', '#ffcccb', '#F8C8DC'] # pinks\n",
    "\n",
    "# Plot the crosstab as a stacked bar plot\n",
    "ax = ct.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(9, 6),\n",
    "    color=colors,\n",
    "    edgecolor='black',\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Style adjustments for the plot\n",
    "plt.title(\"Billing Flag vs Monthly Pricing Tiers\", fontsize=16, weight='bold', color='#333')\n",
    "plt.xlabel(\"Billing Flag\", fontsize=13)\n",
    "plt.ylabel(\"Percentage (%)\", fontsize=13)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=11)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(100))\n",
    "\n",
    "# Add % labels on each stacked bar\n",
    "for container in ax.containers:\n",
    "    values = container.datavalues\n",
    "    labels = [f\"{v:.1f}%\" if v > 0 else \"\" for v in values]\n",
    "    ax.bar_label(container, labels=labels, label_type='center', color='black', fontsize=10, weight='bold')\n",
    "\n",
    "# Customise legend appearance\n",
    "plt.legend(title=\"Pricing Tiers\", fontsize=11, title_fontsize=12, loc=\"upper right\", frameon=False)\n",
    "\n",
    "# Remove frame for a cleaner look\n",
    "plt.box(False)\n",
    "\n",
    "# Adjust spacing for a neat layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f080a",
   "metadata": {},
   "source": [
    "**<u>Observations</u>**\n",
    "- Premium customers show the highest share of discounts (34%).\n",
    "    - Possible targeted retention strategy: keep high-paying customers happy.\n",
    "\n",
    "- Billing issues are concentrated in Basic plans (38%).\n",
    "    - Lower-tier customers may be more sensitive to billing errors.\n",
    "\n",
    "- Partial month occurs across tiers but highest in Premium (37%).\n",
    "    - Suggests churn risk is high even among mid-high paying customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8ee22",
   "metadata": {},
   "source": [
    "**<u>Overall Insight</u>**\n",
    "\n",
    "- Partial-month customers (month-to-month contracts) are the main churn risk group — they’re flexible and leave easily.\n",
    "\n",
    "- Billing issues affect long-term, basic plan customers — not an immediate churn trigger, but could harm satisfaction.\n",
    "\n",
    "- Discounts seem effective, especially for Premium/Platinum tiers, where churn is lower compared to partial-month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600467f",
   "metadata": {},
   "source": [
    "#### 1.3.7. Calculating the average charges per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c369265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column to calculate the average charges per month\n",
    "dataset['average_charges_per_month'] = (dataset['TotalCharges'] / dataset['tenure']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adda174",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding this feature:</u>**\n",
    "\n",
    "- This feature was created to better measure customer value beyond ‘tenure’. ‘TotalCharges’ reflects overall revenue from a customer, however, it is heavily influenced by tenure. ‘average_total_charges’ makes it easier to segment customers by profitability by identifying high-value customers as well as those underutilising services or subscribing only to basic plans.\n",
    "\n",
    "    - If low-value customers churn more -> they may be more price-sensitive -> the company could consider cheaper bundles, discounts, or targeted engagement as retention plans.\n",
    "\n",
    "    - If high-value customers churn more -> business is losing its best customers -> this signals possible dissatisfaction with service quality or perceived cost vs. benefit.\n",
    "\n",
    "    - If churn is evenly distributed -> customer value isn’t the main driver of churn -> so other factors (like contract type or support) might matter more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Churn column to numeric \n",
    "dataset['Churn_encoded'] = dataset['Churn'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# Create bins for average_total_charges\n",
    "dataset['avg_charge_bin'] = pd.qcut(dataset['average_charges_per_month'], q=5, duplicates='drop')\n",
    "\n",
    "# Calculate churn rate per bin\n",
    "churn_rates = dataset.groupby('avg_charge_bin')['Churn_encoded'].mean()\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Plot churn rate by average charge bins\n",
    "churn_rates.plot(kind='bar', edgecolor='black', color='#FF5C8D')\n",
    "\n",
    "# Add title\n",
    "plt.title('Churn Rate Across Average Monthly Charges')\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel('Churn Rate')\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel('Average Monthly Charges (Binned)')\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0b407",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "- Low-paying customers (< $25/month) are more loyal, possibly because they are on basic, affordable plans.\n",
    "\n",
    "- Mid-to-high paying customers (>$60/month) are much more likely to churn, maybe due to:\n",
    "    - Perception of higher cost vs. value\n",
    "    - Competition offering cheaper alternatives\n",
    "    - Extra services not matching expectations\n",
    "\n",
    "In short: this validates the notion that as monthly charges rise, churn risk increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a2e99",
   "metadata": {},
   "source": [
    "#### 1.3.8. Combining 'Contract' & 'tenure'\n",
    "\n",
    "This will tell us how long a customer has stayed, relative to the contract they signed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08f9fa",
   "metadata": {},
   "source": [
    "##### 1.3.8.1. Loyalty flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature 'contract_loyalty'\n",
    "dataset['contract_loyalty'] = (\n",
    "    (dataset['Contract'] != 'Month-to-month') & \n",
    "    (dataset['tenure'] > 12)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c38dfb9",
   "metadata": {},
   "source": [
    "A customer is considered \"loyal\" if:\n",
    "1. Their contract type is NOT 'Month-to-month'\n",
    "2. Their tenure is greater than 12 months\n",
    "\n",
    "The result is converted to integer: \n",
    "- 1: Has long contract and stayed over a year -> likely loyal\n",
    "- 0: Either on month-to-month or hasn’t stayed long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e4e7b",
   "metadata": {},
   "source": [
    "##### 1.3.8.2. Contract progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to convert contract type into numeric length (in months)\n",
    "def contract_length(contract):\n",
    "    if contract == 'Month-to-month':     # Month-to-moth - 1 month\n",
    "        return 1\n",
    "    elif contract == 'One year':         # One year contract - 12 months\n",
    "        return 12\n",
    "    elif contract == 'Two year':         # Two year contract - 24 months\n",
    "        return 24\n",
    "\n",
    "# Apply the function to create new column \"contract_length\"\n",
    "dataset['contract_length'] = dataset['Contract'].apply(contract_length)\n",
    "\n",
    "# Create new column to calculate how far a customer has progressed into their contract\n",
    "dataset['contract_progress'] = (dataset['tenure'] / dataset['contract_length']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e59dc",
   "metadata": {},
   "source": [
    "What it means:\n",
    "- contract_progress = 1.0 -> they finished their contract\n",
    "- **>1.0** -> they renewed their contract\n",
    "- **<1.0** -> they're still within contract\n",
    "\n",
    "This gives a numeric signal to the model about where the customer is in their commitment journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077b6b4",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding these features (contract_loyalty and contract_progress):</u>**\n",
    "\n",
    "These features were engineered to capture contract-related behaviour, since tenure alone does not guarantee commitment, and contract type alone does not prevent churn. By combining the two, the feature encodes a stronger signal of customer stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d242fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin contract_progress into categories (e.g., 0–20%, 20–40%, etc.)\n",
    "dataset['progress_bin'] = pd.cut(dataset['contract_progress'], \n",
    "                                 bins=[0, 0.2, 0.4, 0.6, 0.8, 1.2], \n",
    "                                 labels=['0–20%', '20–40%', '40–60%', '60–80%', '80–120%'])\n",
    "\n",
    "# Calculate churn distribution per bin\n",
    "churn_dist = dataset.groupby('progress_bin')['Churn'].value_counts(normalize=True).mul(100).rename('percentage').reset_index()\n",
    "\n",
    "# Define custom colours for churn = Yes (1) and churn = No (0)\n",
    "colors = {\n",
    "    \"Yes\": '#F8C8DC',  \n",
    "    \"No\":\"#FF5C8D\"   \n",
    "}\n",
    "\n",
    "# Plot churn distribution by contract progress\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Loop through each category (Yes/No)\n",
    "for churn_status in churn_dist['Churn'].unique():\n",
    "    # Filter dataset for the current churn category\n",
    "    subset = churn_dist[churn_dist['Churn'] == churn_status]\n",
    "\n",
    "    # Plot bars for each contract progress bin\n",
    "    plt.bar(subset['progress_bin'], subset['percentage'],  \n",
    "            label=f'Churn: {churn_status}', \n",
    "            alpha=0.9,\n",
    "            color=colors[churn_status], edgecolor='black')  \n",
    "\n",
    "# Add chart title and axis labels\n",
    "plt.title('Churn Distribution by Contract Progress')\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel('Percentage (%)')\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel('Contract Progress')\n",
    "\n",
    "# Add legend with churn categories\n",
    "plt.legend(title='Churn')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287833f",
   "metadata": {},
   "source": [
    "**<u>Interpretation:</u>**\n",
    "1. Early stages (0–20%):\n",
    "    - Churn rate is higher here (~10%) compared to later stages.\n",
    "    - Suggests that a notable number of customers drop out early into their contracts.\n",
    "\n",
    "2. Middle stages (20–80%):\n",
    "    - Churn rate is very low (around 2–3%).\n",
    "    - Most customers stick around once they pass the initial onboarding phase.\n",
    "\n",
    "3. Final stage (80–120%):\n",
    "    - Churn rate jumps up again (~50%).\n",
    "    - Indicates that many customers are leaving as their contracts approach the end or just after the first cycle completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many customers fall into each unique value of 'contract_progress'\n",
    "dataset['contract_progress'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad7b65",
   "metadata": {},
   "source": [
    "#### 1.3.9. How many add-ons does a customer subscribe to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f196a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of service-related columns to analyse\n",
    "service_cols = ['PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "                'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "\n",
    "# Create a new column to count the number of services a customer is subscribed to\n",
    "dataset['ServiceCount'] = (dataset[service_cols] == 'Yes').sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e023f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sorted unique values of 'ServiceCount'\n",
    "print(np.sort(dataset['ServiceCount'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798279fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colour palette\n",
    "colors = ['#FF5C8D', '#F8C8DC'] # pink \n",
    "\n",
    "# Plot churn distribution across service counts\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(data=dataset, x='ServiceCount', hue='Churn', palette=colors, edgecolor='black', saturation=1)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Churn Distribution by Number of Services Subscribed\")\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel(\"Service Count (0–8)\")\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title=\"Churn\")\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68dd49f",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding this feature:</u>**\n",
    "\n",
    "This feature was created to reflect the breadth of customer engagement. \n",
    "- Higher service counts suggest stronger attachment to the provided.\n",
    "- Lower counts may indicate greater churn risk.\n",
    "- The distribution below shows that customers with four or more services are significantly less likely to churn.\n",
    "    - This suggests that broader adoption increases stickiness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5020de1",
   "metadata": {},
   "source": [
    "#### 1.3.10. How expensive was a customer's service relative to how long they stuck around?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7778b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature: charge-to-tenure ratio\n",
    "dataset['charge_tenure_ratio'] = (dataset['MonthlyCharges'] / (dataset['tenure'] + 1)).round(2)\n",
    "\n",
    "# Apply log transformation to reduce skewness in the ratio\n",
    "dataset['charge_tenure_ratio_log'] = np.log1p(dataset['charge_tenure_ratio']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define figure with independent axes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,6), constrained_layout=True)\n",
    "\n",
    "# Left plot: Raw charge-tenure ratio\n",
    "sns.histplot(data=dataset,\n",
    "             x='charge_tenure_ratio',\n",
    "             hue='Churn',\n",
    "             bins=30,\n",
    "             kde=True,\n",
    "             palette={'Yes': '#FF9933', 'No': '#FF5C8D'},\n",
    "             alpha=0.3,\n",
    "             ax=axes[0])\n",
    "# Add title\n",
    "axes[0].set_title(\"Churn Distribution by Charge-Tenure Ratio\", fontsize=14, weight='bold')\n",
    "\n",
    "# Add x-label\n",
    "\n",
    "axes[0].set_xlabel(\"Charge-Tenure Ratio\", fontsize=12)\n",
    "\n",
    "# Add y-label\n",
    "axes[0].set_ylabel(\"Count\", fontsize=12)\n",
    "\n",
    "\n",
    "# Right plot: Log-transformed ratio\n",
    "sns.histplot(data=dataset,\n",
    "             x='charge_tenure_ratio_log',\n",
    "             hue='Churn',\n",
    "             bins=30,\n",
    "             kde=True,\n",
    "             palette={'Yes': '#FF9933', 'No': '#FF5C8D'},\n",
    "             alpha=0.3,\n",
    "             ax=axes[1])\n",
    "# Add title\n",
    "axes[1].set_title(\"Churn Distribution by Charge-Tenure Ratio (Log)\", fontsize=14, weight='bold')\n",
    "\n",
    "# Add x-label\n",
    "axes[1].set_xlabel(\"Charge-Tenure Ratio (Log)\", fontsize=12)\n",
    "\n",
    "# Add y-label\n",
    "axes[1].set_ylabel(\"Count\", fontsize=12)\n",
    "\n",
    "\n",
    "# Shared legend (outside, right side)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title=\"Churn\", fontsize=12, title_fontsize=13,\n",
    "           loc=\"center left\", bbox_to_anchor=(1.0, 0.5), frameon=False)\n",
    "\n",
    "# Display plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28132673",
   "metadata": {},
   "source": [
    "**<u>Reasoning behind adding this feature:</u>**\n",
    "\n",
    "'charge_tenure_ratio\" was introduced as a way to measure how expensive a customer’s plan is relative to their length of stay.\n",
    "\n",
    "- Customers with high ratios are paying a large amount despite short tenures. This signifies potential signals of dissatisfaction or churn risk.\n",
    "-  Conversely, low ratios reflect customers’ spending costs across longer stays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cbdbc3",
   "metadata": {},
   "source": [
    "#### 1.3.11. Could a deeply engaged customer on a longer plan be less likely to leave? (Many services + one‑year contract)\n",
    "\n",
    "- “Deeply engaged” -> ServicesCount is high\n",
    "- “On a longer plan” -> Contract is either 'One year' or 'Two year'\n",
    "\n",
    "What metric is used to decide when a customer is deeply engaged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many customers fall into each possible number of subscribed services (0–8)\n",
    "dataset['ServiceCount'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04635a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics of ServiceCount\n",
    "dataset['ServiceCount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b57e86",
   "metadata": {},
   "source": [
    "**<u>Summary of What the Data Says:</u>**\n",
    "- Most common values: 1–3 services\n",
    "- Median (50%): 3 services\n",
    "- 75th percentile: 5 services\n",
    "- Max: 8 services\n",
    "- Only 25% of customers have more than 5 services\n",
    "- Only ~21% (1495/7032) have 6 or more\n",
    "\n",
    "\n",
    "To define deep engagement, it makes sense to pick above-average users, not average ones.\n",
    "| Threshold | Label                        | % of customers |\n",
    "| --------- | ---------------------------- | -------------- |\n",
    "| ≥ 4       | Slightly above median        | \\~39%          |\n",
    "| ≥ 5       | 75th percentile or higher    | \\~25%          |\n",
    "| ≥ 6       | Top quartile (“power users”) | \\~21%          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc08c362",
   "metadata": {},
   "source": [
    "This captures the top ~25% of customers with many subscribed services -> engagement and customers on 1-year or 2-year contracts = highly loyal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature called 'high_engagement_loyalty'\n",
    "dataset['high_engagement_loyalty'] = (\n",
    "    (dataset['ServiceCount'] >= 5) & \n",
    "    (dataset['contract_length'] >= 12)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032863af",
   "metadata": {},
   "source": [
    "A customer is considered \"high engagement & loyal\" if BOTH conditions hold:\n",
    "1. They subscribe to 5 or more services (ServiceCount >= 5)\n",
    "2. Their contract length is at least 12 months (contract_length >= 12)\n",
    "\n",
    "The result is cast to an integer:\n",
    "- 1: High engagement & loyal\n",
    "- 0: Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Create countplot of churn distribution across the new 'high_engagement_loyalty' feature\n",
    "sns.countplot(data=dataset, x='high_engagement_loyalty', hue='Churn', palette=colors, edgecolor='black', saturation=1)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Churn Distribution by High Engagement & Loyalty\")\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel(\"High Engagement Loyalty (0 = No, 1 = Yes)\")\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title=\"Churn\")\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e237fb8a",
   "metadata": {},
   "source": [
    "#### Note: If you are re-running this notebook during evaluation, first go to section **3. Feature Engineering for Improving Evaluation Metrics**. After completing that section, return here and continue running this cell and the ones that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643e681",
   "metadata": {},
   "source": [
    "#### 1.3.12. Dropping features that are no longer needed\n",
    "\n",
    "These are excluded because they are:\n",
    "- Identifiers ('customerID') -> do not help prediction\n",
    "- Raw features already transformed into engineered ones: ('MonthlyCharges', 'TotalCharges', 'charge_diff', 'charge_tenure_ratio', 'contract_length')\n",
    "- Created for visualisation purposes: ('avg_charge_bin', 'progress_bin')\n",
    "- The target variable ('Churn', 'Churn_encoded') -> must be separated from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfd3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the list of features to remove before modeling\n",
    "features_to_remove = ['customerID', 'MonthlyCharges', 'TotalCharges', \n",
    "                      'avg_charge_bin', 'charge_diff', 'charge_tenure_ratio', \n",
    "                      'contract_length', 'Churn', 'Churn_encoded', 'progress_bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77593e96",
   "metadata": {},
   "source": [
    "### 1.4. Defining the matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the matrix of features\n",
    "X_untransformed = dataset.drop(columns=features_to_remove, axis=1)\n",
    "\n",
    "# Store the name of the features in a list \n",
    "feature_names = X_untransformed.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the new feature matrix\n",
    "X_untransformed.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the shape of the feature matrix\n",
    "X_untransformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde25bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display concise information about the feature matrix\n",
    "X_untransformed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b390f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all categorical columns (dtype = 'object') from X_untransformed\n",
    "cat_cols = X_untransformed.select_dtypes(include='object')\n",
    "\n",
    "# Loop through each categorical column and display its unique values\n",
    "for col in cat_cols:\n",
    "    # Print column name\n",
    "    print(f\"The values for {col} are:\")\n",
    "    # Show all unique categories in that column\n",
    "    print(X_untransformed[col].unique())\n",
    "    # Print separator line for readability\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f49407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all categorical columns with dtype = 'category' (different from 'object')\n",
    "cat_cols_2 = X_untransformed.select_dtypes(include='category')\n",
    "\n",
    "# Loop through each categorical column and display its unique values\n",
    "for col in cat_cols_2:\n",
    "    # Print column name\n",
    "    print(f\"The values for {col} are:\")\n",
    "    # Show unique categories in that column\n",
    "    print(X_untransformed[col].unique())\n",
    "    # Separator line for clarity\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e259f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique values in each column of X_untransformed\n",
    "X_untransformed.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable 'y_df' from the original dataset\n",
    "y_df = dataset['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f9ad14",
   "metadata": {},
   "source": [
    "### 1.5. Splitting the data into training and test sets\n",
    "\n",
    "- X_untransformed: feature set\n",
    "- y_df: target labels (Churn)\n",
    "- stratify=y_df -> ensures class balance is preserved in both sets\n",
    "- test_size=0.3 -> 30% of the data for testing, 70% for training\n",
    "- random_state=42 -> ensures reproducibility of the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52261026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training and test sets in a ratio of 70:30 and applying stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_untransformed, y_df, stratify=y_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb9d2e",
   "metadata": {},
   "source": [
    "### 1.6. Encoding the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae873b47",
   "metadata": {},
   "source": [
    "#### 1.6.1. Dividing the data into feature groups by type for effective encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b29b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary features: categorical columns with only two categories (Yes/No, Male/Female, etc.)\n",
    "binary_features = [\"gender\", \"Partner\", \"Dependents\", \"PhoneService\",  \n",
    "                   \"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\", \n",
    "                   \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \n",
    "                   \"StreamingMovies\", \"PaperlessBilling\"]\n",
    "\n",
    "# Ordinal features: categorical variables with a natural order (e.g., contract length, tenure bins, pricing tiers)\n",
    "ordinal_features = [\"Contract\", \"tenure_bin\", \"monthly_pricing_tiers\"]\n",
    "\n",
    "# Nominal features: categorical variables without an inherent order (e.g., Internet type, payment method, billing flag) - \n",
    "# (contract_paperless commented out for now, but can be added if needed during evaluation)\n",
    "# nominal_features = [\"InternetService\", \"PaymentMethod\", \"billing_flag\", \"contract_paperless\"]\n",
    "nominal_features = [\"InternetService\", \"PaymentMethod\", \"billing_flag\"]\n",
    "\n",
    "# Numeric features: automatically select all numeric columns\n",
    "numeric_features = X_untransformed.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display numeric features\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eacbb76",
   "metadata": {},
   "source": [
    "#### 1.6.2. Applying label, ordinal, and one-hot encoding based on feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398c037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Separate groups\n",
    "# Identify numeric features explicitly (not in ordinal, binary, or nominal groups)\n",
    "num_features = [col for col in X_train.columns \n",
    "                if col not in ordinal_features + binary_features + nominal_features]\n",
    "\n",
    "# Ordinal Encoding: For ordered categorical features (Contract, tenure_bin, monthly_pricing_tiers) \n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "# Fit-transform on training data\n",
    "X_train_ord = pd.DataFrame(\n",
    "    ordinal_encoder.fit_transform(X_train[ordinal_features]),\n",
    "    columns=ordinal_features,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# Transform on test data\n",
    "X_test_ord = pd.DataFrame(\n",
    "    ordinal_encoder.transform(X_test[ordinal_features]),\n",
    "    columns=ordinal_features,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "\n",
    "# Label Encoding: For binary features (Yes/No type variables)\n",
    "X_train_bin = X_train[binary_features].copy()\n",
    "X_test_bin = X_test[binary_features].copy()\n",
    "\n",
    "for col in binary_features:\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    # Fit + transform on training data\n",
    "    X_train_bin[col] = le.fit_transform(X_train[col])\n",
    "\n",
    "    # Only transform on testing data\n",
    "    X_test_bin[col] = le.transform(X_test[col])\n",
    "\n",
    "\n",
    "# One-Hot Encoding: For unordered categorical variables \n",
    "ohe_full = OneHotEncoder(drop=None, handle_unknown=\"ignore\", sparse_output=False)     # full version keeps all categories\n",
    "ohe_drop = OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False)  # drop=\"first\" version avoids multicollinearity (for linear models)\n",
    "\n",
    "\n",
    "# Full OHE (for tree models)\n",
    "X_train_nom_full = pd.DataFrame(\n",
    "    ohe_full.fit_transform(X_train[nominal_features]),\n",
    "    columns=ohe_full.get_feature_names_out(nominal_features),\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_nom_full = pd.DataFrame(\n",
    "    ohe_full.transform(X_test[nominal_features]),\n",
    "    columns=ohe_full.get_feature_names_out(nominal_features),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Drop-first OHE (for linear models)\n",
    "X_train_nom_drop = pd.DataFrame(\n",
    "    ohe_drop.fit_transform(X_train[nominal_features]),\n",
    "    columns=ohe_drop.get_feature_names_out(nominal_features),\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_nom_drop = pd.DataFrame(\n",
    "    ohe_drop.transform(X_test[nominal_features]),\n",
    "    columns=ohe_drop.get_feature_names_out(nominal_features),\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Numeric features (kept as-is)\n",
    "X_train_num = X_train[num_features].copy()\n",
    "X_test_num = X_test[num_features].copy()\n",
    "\n",
    "# Combine everything back into model-ready datasets\n",
    "\n",
    "# For tree-based models (can handle redundant categories -> using full OHE)\n",
    "X_train_tree = pd.concat([X_train_num, X_train_ord, X_train_bin, X_train_nom_full], axis=1)\n",
    "X_test_tree  = pd.concat([X_test_num,  X_test_ord,  X_test_bin,  X_test_nom_full], axis=1)\n",
    "\n",
    "# For linear models (avoid multicollinearity -> using drop-first OHE)\n",
    "X_train_linear = pd.concat([X_train_num, X_train_ord, X_train_bin, X_train_nom_drop], axis=1)\n",
    "X_test_linear  = pd.concat([X_test_num,  X_test_ord,  X_test_bin,  X_test_nom_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bfca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values after encoding\n",
    "print(X_test_linear.isna().sum())\n",
    "print(X_test_tree.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d2a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of processed datasets - Tree dataset uses full one-hot encoding (many more columns)\n",
    "print(f\"One-hot encoded shape for X_train_tree: {X_train_tree.shape}\")  \n",
    "print(f\"Label encoded shape for X_train_linear: {X_train_linear.shape}\") \n",
    "\n",
    "# Check the shape of processed datasets - Linear dataset uses drop-first encoding (fewer columns to avoid multicollinearity)\n",
    "print(f\"One-hot encoded shape for X_test_tree: {X_test_tree.shape}\")  \n",
    "print(f\"Label encoded shape for X_test_linear: {X_test_linear.shape}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ab828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the number of rows after splitting into train/test still matches the original dataset\n",
    "print(\"Total rows after split:\", len(X_train_tree) + len(X_test_tree))\n",
    "print(\"Original rows:\", len(X_untransformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfb797",
   "metadata": {},
   "source": [
    "#### 1.6.3. Encoding the dependent variable vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48c442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dependent variable vector\n",
    "y = dataset['Churn'].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c014d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialise LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform on training labels\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "# Transform test labels using the same mapping\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print encoded results of y-train\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b175ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print encoded results of y-test\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114df5c",
   "metadata": {},
   "source": [
    "### 1.7. Feature scaling\n",
    "\n",
    "The feature matrix for tree-based models will not be scaled, but for linear models, the charge_tenure_ratio_log variable will be scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30e6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialise StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform training data: Scaling only 'charge_tenure_ratio_log'\n",
    "X_train_linear['charge_tenure_ratio_log'] = scaler.fit_transform(\n",
    "    X_train_linear[['charge_tenure_ratio_log']]\n",
    ")\n",
    "# Transform testing data: Scaling only 'charge_tenure_ratio_log'\n",
    "X_test_linear['charge_tenure_ratio_log'] = scaler.transform(\n",
    "    X_test_linear[['charge_tenure_ratio_log']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3aa77",
   "metadata": {},
   "source": [
    "### 1.8. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f1cec",
   "metadata": {},
   "source": [
    "#### 1.8.1. Observing correlations between features and the target variable  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets back to dataframes\n",
    "X_train_tree = pd.DataFrame(X_train_tree)\n",
    "X_train_linear = pd.DataFrame(X_train_linear)\n",
    "y_train_df = pd.DataFrame(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc502ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many columns are of each dtype in X_train_tree\n",
    "X_train_tree.dtypes.value_counts()\n",
    "\n",
    "# Convert all columns in X_train_tree to numeric types.\n",
    "X_train_tree = X_train_tree.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Count how many columns are of each dtype in X_train_linear\n",
    "X_train_linear.dtypes.value_counts()\n",
    "\n",
    "# Convert all columns in X_train_linear to numeric types.\n",
    "X_train_linear = X_train_linear.apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5192588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm shape of X_train_tree\n",
    "print(X_train_tree.shape)\n",
    "\n",
    "# Confirm shape of X_train_linear\n",
    "print(X_train_linear.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13189e3",
   "metadata": {},
   "source": [
    "##### 1.8.1.1. Checking for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Defining the function to calculate the Variance Inflation Factor (VIF)\n",
    "def calculate_vif(data):\n",
    "\n",
    "    # Prepare empty DataFrame to store results\n",
    "    vif_data = pd.DataFrame()\n",
    "\n",
    "    # Store feature names\n",
    "    vif_data['feature'] = data.columns\n",
    "    \n",
    "    # Compute VIF for each column\n",
    "    vif_data['VIF'] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "\n",
    "    # Return results\n",
    "    return vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbc1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display VIF scores for tree matrix\n",
    "print(\"Tree features VIF:\")\n",
    "display(calculate_vif(X_train_tree))\n",
    "\n",
    "# Display VIF scores for linear matrix\n",
    "print(\"Linear features VIF:\")\n",
    "display(calculate_vif(X_train_linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b936dc",
   "metadata": {},
   "source": [
    "**<u>General VIF thresholds:</u>**\n",
    "- VIF < 5 -> No problem, safe to keep.\n",
    "- VIF 5–10 -> Moderate multicollinearity, it's best to be cautious.\n",
    "- VIF > 10 -> High multicollinearity, should consider dropping or combining features.\n",
    "\n",
    "**<u>Key Observations from table:</u>**\n",
    "\n",
    "<u>VIF Values < 5 (low collinearity - safe to keep):</u>\n",
    "\n",
    "- SeniorCitizen, gender, Partner, Dependents, MultipleLines, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, PaperlessBilling, PaymentMethod_Credit card (automatic), PaymentMethod_Electronic check, PaymentMethod_Mailed check, billing_flag_discount, billing_flag_ok, billing_flag_partial_month, and high_engagement_loyalty.\n",
    "\n",
    "<u>VIF Values (5-10) (moderate collinearity - monitor, might combine/drop later):</u>\n",
    "\n",
    "- charge_tenure_ratio_log, contract_progress, monthly_pricing_tiers, InternetService_Fiber optic, and InternetService_No.\n",
    "\n",
    "<u>VIF Values > 10 (High collinearity - needs action):</u>\n",
    "\n",
    "- tenure, contract_loyalty, Contract, tenure_bin, and PhoneService."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24559ba7",
   "metadata": {},
   "source": [
    "**<u>Strategies:</u>**\n",
    "1. For Logistic Regression / linear models:\n",
    "    - Drop or carefully choose between: tenure vs tenure_bin vs contract_progress vs contract_loyalty as they overlap heavily:\n",
    "        - Keep tenure_bin and drop tenure. Keep contract_progress and drop contract_loyalty.\n",
    "    - PhoneService vs MultipleLines are very correlated:\n",
    "        - Keep MultipleLines, drop raw PhoneService.\n",
    "    - charge_tenure_ratio_log vs tenure/MonthlyCharges_log:\n",
    "        - Keep charge_tenure_ratio_log, drop tenure + MonthlyCharges_log to reduce redundancy.\n",
    "    - Drop contract_loyalty, keep Contract.\n",
    "\n",
    "\n",
    "2. For Tree-based models:\n",
    "    - Keep all of them. Trees don’t break from collinearity, they just ignore redundant splits.\n",
    "        - Keep everything if performance improves — no need to drop unless interpretability is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaa0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Uncomment during evaluation stage ---\n",
    "# redundant_cols = ['MonthlyCharges_log', 'TotalCharges_log', 'average_charges_per_month',\n",
    "#                   'ServiceCount', 'security_bundle', 'contract_paperless_Month-to-month_Yes', 'contract_paperless_One year_No',\n",
    "#                   'contract_paperless_One year_Yes', 'contract_paperless_Two year_No', 'contract_paperless_Two year_Yes',\n",
    "#                   'streaming_bundle']\n",
    "\n",
    "# Defining the list of features to drop  (Comment out during evaluation stage)\n",
    "redundant_cols = ['MonthlyCharges_log', 'TotalCharges_log', 'average_charges_per_month',\n",
    "                  'ServiceCount']\n",
    "\n",
    "# Drop features that are high in multicollinearity in the linear feature matrix\n",
    "X_train_linear = X_train_linear.drop(columns=redundant_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ed29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store both encoded training datasets in a list for iteration\n",
    "encoded_datasets = [X_train_tree, X_train_linear]\n",
    "\n",
    "# Loop through each encoded dataset\n",
    "for i in encoded_datasets:\n",
    "    # Check which dataset is being processed \n",
    "    if i is X_train_linear:\n",
    "        print(\"VIF Without 'drop_first' (After Dropping Redundant Cols)\")\n",
    "        display(calculate_vif(i)) # Calculate and display VIF values\n",
    "    else:\n",
    "        print(\"VIF With 'drop_first' (After Dropping Redundant Cols)\")\n",
    "        display(calculate_vif(i)) # Calculate and display VIF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Make a copy of the training dataset for tree-based models\n",
    "corr_data_tree = X_train_tree.copy()\n",
    "\n",
    "# Add the target column 'Churn' to the dataset to analyse correlations\n",
    "corr_data_tree[\"Churn\"] = y_train  \n",
    "\n",
    "# Compute the pairwise correlation matrix for all features + target\n",
    "corr_matrix_tree = corr_data_tree.corr()\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(21,18)) # Make the heatmap large for readability\n",
    "\n",
    "# Plot the heatmap of correlations\n",
    "sns.heatmap(\n",
    "    corr_matrix_tree.corr(),\n",
    "    cmap=\"PuRd\", \n",
    "    center=0,\n",
    "    annot=False,\n",
    "    linewidths=0.1,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Feature Correlation Heatmap (Full Matrix)\", fontsize=14, pad=12)\n",
    "\n",
    "# Display heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd53a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Make a copy of the training dataset for tree-based models\n",
    "corr_data_linear = X_train_linear.copy()\n",
    "\n",
    "# Add the target column 'Churn' to the dataset to analyse correlations\n",
    "corr_data_linear[\"Churn\"] = y_train  \n",
    "\n",
    "# Compute the pairwise correlation matrix for all features + target\n",
    "corr_matrix_linear = corr_data_linear.corr()\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(20,18)) # Make the heatmap large for readability\n",
    "\n",
    "# Plot the heatmap of correlations\n",
    "sns.heatmap(\n",
    "    corr_matrix_linear.corr(),\n",
    "    cmap=\"PuRd\",  # magma RdPu PuRd\n",
    "    center=0,\n",
    "    annot=False,\n",
    "    linewidths=0.1,\n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Feature Correlation Heatmap (Streamlined Matrix)\", fontsize=14, pad=12)\n",
    "\n",
    "# Display heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of this dataset\n",
    "Xy_train_tree = X_train_tree.copy()\n",
    "Xy_train_tree[\"Churn\"] = y_train # Add target back\n",
    "\n",
    "# Compute correlation with target\n",
    "correlations_tree = Xy_train_tree.corr()[\"Churn\"].sort_values(ascending=False).round(2)\n",
    "\n",
    "# Print correlation values\n",
    "print(correlations_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddcc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of this dataset\n",
    "Xy_train_linear = X_train_linear.copy()\n",
    "\n",
    "Xy_train_linear[\"Churn\"] = y_train # Add target back\n",
    "\n",
    "# Compute correlation with target\n",
    "correlations_linear = Xy_train_linear.corr()[\"Churn\"].sort_values(ascending=False).round(2)\n",
    "\n",
    "# Print correlation values\n",
    "print(correlations_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d83989",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "1. Top correlators are: \n",
    "- charge_tenure_ratio_log (0.48)\n",
    "- OnlineSecurity_No (0.34),\n",
    "- TechSupport_No (0.34), and\n",
    "- InternetService_Fibre optic (0.31). \n",
    "\n",
    "2. The strong negative correlators with churn are: \n",
    "- Contract (-0.40) \n",
    "- contract_loyalty (-0.38), and \n",
    "- tenure (-0.35) \n",
    "\n",
    "Intuition is limited as nonlinear relationships and interaction effects are ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f2b851",
   "metadata": {},
   "source": [
    "#### 1.8.2. Feature selection using statistical tests for relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0f62c",
   "metadata": {},
   "source": [
    "##### 1.8.2.1. Calculating the mutual information scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a07df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Defining the function to calculate the mutual information scores for each matrix \n",
    "def calculate_mi_scores(feature_matrix, dependent_vector):\n",
    "\n",
    "    # Compute the mutual information score for each feature\n",
    "    mi_scores = mutual_info_classif(feature_matrix, dependent_vector)\n",
    "\n",
    "    # Convert results into a Pandas Series for readability\n",
    "    mi_scores = pd.Series(mi_scores, index=feature_matrix.columns).sort_values(ascending=False).round(2)\n",
    "\n",
    "    # Display the feature importance scores\n",
    "    return mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mutual information scores between features (X_train_tree) and target (y_train)\n",
    "mi_scores_tree = calculate_mi_scores(feature_matrix=X_train_tree, dependent_vector=y_train)\n",
    "display(mi_scores_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2457b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mutual information scores between features (X_train_linear) and target (y_train)\n",
    "mi_scores_linear = calculate_mi_scores(feature_matrix=X_train_linear, dependent_vector=y_train)\n",
    "display(mi_scores_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff568386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom palette for plots\n",
    "mi_scores_palette = ['#FF5C8D', '#F8C8DC'] # pinks\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "# Create barplot\n",
    "sns.barplot(x=mi_scores_tree, y=mi_scores_tree.index, palette=mi_scores_palette, edgecolor='black', saturation=1)  \n",
    "\n",
    "# Add title\n",
    "plt.title(\"Mutual Information Scores of Full Matrix Features\")\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel(\"Score\")\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c4171",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "1. Strongest predictive signals for tree models are:\n",
    "- charge_tenure_ratio_log\n",
    "- Contract\n",
    "- contract_progress and \n",
    "- contract_loyalty \n",
    "\n",
    "2. Weakest informative variables are: \n",
    "- MultipleLines\n",
    "- gender \n",
    "- PhoneService and \n",
    "- certain categories of ‘billing_flag’ \n",
    "\n",
    "These are marked as candidates for removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29816057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with seaborn (palette goes here, not in title)\n",
    "plt.figure(figsize=(20, 18))\n",
    "# Create barplot\n",
    "sns.barplot(x=mi_scores_linear, y=mi_scores_linear.index, palette=mi_scores_palette, edgecolor='black', saturation=1)  \n",
    "# Add title\n",
    "plt.title(\"Mutual Information Scores of Streamlined Matrix Features\")\n",
    "# Add x-label\n",
    "plt.xlabel(\"Score\")\n",
    "# Add y-label\n",
    "plt.ylabel(\"Features\")\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7778f82",
   "metadata": {},
   "source": [
    "## COME BACK AND ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eba70d",
   "metadata": {},
   "source": [
    "#### 1.8.3. Feature selection using model-based feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53919e3b",
   "metadata": {},
   "source": [
    "##### 1.8.3.1. Using a random forest model to test the tree feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_tree, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train_tree.columns)\n",
    "importances.sort_values(ascending=False).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ca2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom palette\n",
    "importances_palette = ['#FF5C8D', '#F8C8DC'] # PINKS\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(20, 18))\n",
    "\n",
    "# Create barplot\n",
    "sns.barplot(x=importances, y=importances.index, palette=importances_palette, edgecolor='black', saturation=1)\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel(\"Importance\")\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel(\"Features\")\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f100fd",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "\n",
    "- The random forest model highlights that customer tenure and payment behaviour (e.g., tenure length, average charges, and charge-to-tenure ratios) are the strongest signals of churn risk. \n",
    "\n",
    "- This suggests that long-term loyalty and stable payment patterns are critical for retention, and disruptions in these areas are strong warning signs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67f2b5f",
   "metadata": {},
   "source": [
    "##### 1.8.3.2. Using a logistic regression model to test the linear feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ae1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')\n",
    "log_reg.fit(X_train_linear, y_train)\n",
    "\n",
    "# Extract feature importance from coefficients\n",
    "linear_importances = pd.Series(log_reg.coef_[0], index=X_train_linear.columns)\n",
    "\n",
    "# Sort by absolute importance (magnitude of effect, not direction)\n",
    "linear_importances = linear_importances.abs().sort_values(ascending=False).round(2)\n",
    "\n",
    "print(importances) # Print importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc605588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=importances.values,           # Importance values\n",
    "    y=importances.index,            # Feature names\n",
    "    palette=importances_palette,\n",
    "    edgecolor='black',              # Edge color for bars\n",
    "    linewidth=1,                    # Thickness of the edges\n",
    "    saturation=1\n",
    ")\n",
    "\n",
    "# Add title \n",
    "plt.title(\"Feature Importance from Logistic Regression\")\n",
    "\n",
    "# Add x-label\n",
    "plt.xlabel(\"Absolute Coefficient Value\")\n",
    "\n",
    "# Add y-label\n",
    "plt.ylabel(\"Feature\")\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b10c4e",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "\n",
    "- The logistic regression model assigns more weight to service-related attributes such as contract type and whether the customer has phone services or multiple lines.\n",
    "\n",
    "- This indicates that contractual commitments and service bundles are more directly tied to churn from a linear perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47d81e",
   "metadata": {},
   "source": [
    "#### 1.8.4. Dropping features of low importance \n",
    "\n",
    "To help reduce overfitting and improve model performance, features that performed poorly or added noise across all analyses during the feature selection phase will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82072a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to drop from the tree feature matrix \n",
    "remove_for_trees = ['tenure_bin', 'gender', 'PhoneService', 'StreamingTV', 'StreamingMovies', 'DeviceProtection',\n",
    "                    'OnlineBackup', 'MultipleLines', 'PaymentMethod_Mailed check',\n",
    "                    'PaymentMethod_Credit card (automatic)', 'InternetService_DSL']\n",
    "\n",
    "# List of features to drop from the linear feature matrix \n",
    "remove_for_linear = ['tenure', 'contract_progress', 'gender', 'PhoneService', 'StreamingTV', 'StreamingMovies', 'DeviceProtection',\n",
    "                    'OnlineBackup', 'MultipleLines', 'PaymentMethod_Mailed check',\n",
    "                    'PaymentMethod_Credit card (automatic)']\n",
    "\n",
    "# Drop less useful features for tree-based models\n",
    "X_train_tree = X_train_tree.drop(columns=remove_for_trees) \n",
    "\n",
    "# Drop multicollinear/redundant features for linear models\n",
    "X_train_linear = X_train_linear.drop(columns=remove_for_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffd700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tree-based models\n",
    "selected_tree_features = X_train_tree.columns            # Get list of tree features kept for training \n",
    "X_test_tree = X_test_tree[selected_tree_features]        # Align tree test set with selected features \n",
    "\n",
    "# For linear models\n",
    "selected_linear_features = X_train_linear.columns        # Get list of linear features kept for training\n",
    "X_test_linear = X_test_linear[selected_linear_features]  # Align linear test set with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tree = X_test_tree[selected_tree_features]       # Keep only tree-model features in test set to match training columns\n",
    "X_test_linear = X_test_linear[selected_linear_features] # Keep only linear-model features in test set to match training columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2785059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_vif(X_train_linear)) # Final check for multicollinear/redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe1e94",
   "metadata": {},
   "source": [
    "**<u>Observations:</u>**\n",
    "\n",
    "- Multicollinearity has been eliminated in the linear feature matrix.\n",
    "- All variables have safe to moderate VIF values now.\n",
    "- These are now safe to put into the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c000b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training set size for tree model\n",
    "print(\"Shape for X_train_tree:\", X_train_tree.shape)\n",
    "\n",
    "# Check test set size for tree model\n",
    "print(\"Shape for X_test_tree:\", X_test_tree.shape)\n",
    "\n",
    "# Check training set size for linear model\n",
    "print(\"Shape for X_train_linear:\", X_train_linear.shape)\n",
    "\n",
    "# Check test set size for linear model\n",
    "print(\"Shape for X_test_linear:\", X_test_linear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names\n",
    "import joblib\n",
    "\n",
    "# Save tree-model feature names for later use\n",
    "joblib.dump(X_train_tree.columns.tolist(), '../saved_data/feature_names/tree_feature_names.pkl')\n",
    "\n",
    "# Save linear-model feature names for later use\n",
    "joblib.dump(X_train_linear.columns.tolist(), '../saved_data/feature_names/linear_feature_names.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac74fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a numpy array \n",
    "X_train_tree = X_train_tree.values \n",
    "X_train_linear = X_train_linear.values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a4553",
   "metadata": {},
   "source": [
    "### 1.9. Using SMOTE (Synthetic Minority Oversampling Technique) to handle imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3715f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE with a fixed random state for reproducibility\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to resample training data for tree-based model\n",
    "X_train_resampled_tree, y_train_resampled_tree = smote.fit_resample(X_train_tree, y_train)\n",
    "\n",
    "# Apply SMOTE to resample training data for linear model\n",
    "X_train_resampled_linear, y_train_resampled_linear = smote.fit_resample(X_train_linear, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf97a9",
   "metadata": {},
   "source": [
    "**<u>Reasons for Using Oversampling:</u>**\n",
    "- This is a mild to moderate imbalance (not extreme like 95:5), so both oversampling and undersampling are viable options — but each has trade-offs.\n",
    "    - The dataset is not huge -> oversampling won’t be too slow\n",
    "    - Minority class is still decently sized -> SMOTE can create diverse synthetic samples\n",
    "    - Undersampling would discard too much useful data (removing from 4922 to ~1869) -> risk of underfitting and poor generalization.\n",
    "\n",
    "**<u>Considerations for Evaluation Stage:</u>**\n",
    "- During evaluation, use metrics that handle the imbalance: precision, recall, f1-score, auc-roc, and confusion matrices; avoid only using accuracy.\n",
    "- Consider implementing ensemble methods like RandomForest or XGBoost; these perform better on imbalanced datasets.\n",
    "- Possibly use other models or algorithms that handle imbalance better (like class_weight='balanced' in logistic regression, decision trees, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Check class distribution after resampling\n",
    "print(Counter(y_train_resampled_tree))\n",
    "print(Counter(y_train_resampled_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save resampled training sets\n",
    "joblib.dump(X_train_resampled_tree, '../saved_data/train_test_data/X_train_tree.pkl')       # Tree-model training features\n",
    "joblib.dump(X_train_resampled_linear, '../saved_data/train_test_data/X_train_linear.pkl')   # Linear-model training features\n",
    "\n",
    "# Save test feature sets\n",
    "joblib.dump(X_test_tree, '../saved_data/train_test_data/X_test_tree.pkl')                   # Tree-model test features\n",
    "joblib.dump(X_test_linear, '../saved_data/train_test_data/X_test_linear.pkl')               # Linear-model test features\n",
    "\n",
    "# Save resampled training labels\n",
    "joblib.dump(y_train_resampled_tree, '../saved_data/train_test_data/y_train_tree.pkl')       # Tree-model training labels\n",
    "joblib.dump(y_train_resampled_linear, '../saved_data/train_test_data/y_train_linear.pkl')   # Linear-model training labels\n",
    "\n",
    "# Save test labels\n",
    "joblib.dump(y_test, '../saved_data/train_test_data/y_test.pkl')                             # Unforeseen labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86666890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save original copies\n",
    "\n",
    "X_train_tree_original = X_train_resampled_tree.copy()       # Backup tree-model training features\n",
    "X_train_linear_original = X_train_resampled_linear.copy()   # Backup linear-model training features\n",
    "\n",
    "X_test_tree_original = X_test_tree.copy()                   # Backup tree-model test features\n",
    "X_test_linear_original = X_test_linear.copy()               # Backup linear-model test features\n",
    "\n",
    "y_train_tree_original = y_train_resampled_tree.copy()       # Backup tree-model training labels\n",
    "y_train_linear_original = y_train_resampled_linear.copy()   # Backup linear-model training labels\n",
    "\n",
    "y_test_original = y_test.copy()                             # Backup test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset before new model\n",
    "\n",
    "X_train_tree = X_train_tree_original.copy()         # Restore tree-model training features\n",
    "X_train_linear = X_train_linear_original.copy()     # Restore linear-model training features\n",
    "\n",
    "X_test_tree = X_test_tree_original.copy()           # Restore tree-model test features\n",
    "X_test_linear = X_test_linear_original.copy()       # Restore linear-model test features\n",
    "\n",
    "y_train_tree = y_train_tree_original.copy()         # Restore tree-model training labels\n",
    "y_train_linear = y_train_linear_original.copy()     # Restore linear-model training labels\n",
    "\n",
    "y_test = y_test_original.copy()                     # Restore test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# --- Save scaled data ---\n",
    "# joblib.dump(scaler, '../saved_data/scaler/scaler.pkl')  # in modelling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3208c",
   "metadata": {},
   "source": [
    "## 2. Building the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fd4c27",
   "metadata": {},
   "source": [
    "Preserving the original data when testing the multiple models so that data leakage, transformation effects, or accidental mutations don’t interfere with the comparisons. Before training a new model, the working variables will be reset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab924e",
   "metadata": {},
   "source": [
    "### 2.1. Building a reusable pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_classifier = LogisticRegression(random_state=42,  C=1.0, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# svc_classifier = SVC(kernel=\"linear\", C=1, random_state=42)\n",
    "\n",
    "# dt_classifier = DecisionTreeClassifier(max_depth=5, min_samples_split=10, \n",
    "#                                        min_samples_leaf=5, criterion='entropy', random_state=42)\n",
    "\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=10, \n",
    "#                                        min_samples_leaf=5, criterion='entropy', random_state=42)\n",
    "\n",
    "# xgb_classifier = XGBClassifier(random_state=42, n_jobs=-1, n_estimators=300, \n",
    "#                                max_depth=6, learning_rate=0.1, reg_alpha=0.1, reg_lambda=1.0)\n",
    "\n",
    "# lgb_classifier = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, num_leaves=31, random_state=42)\n",
    "\n",
    "# cat_classifier = CatBoostClassifier(random_seed=42, iterations=300, depth=6, learning_rate=0.1, verbose=0)\n",
    "\n",
    "# models = [lr_classifier, svc_classifier, dt_classifier, rf_classifier, xgb_classifier, lgb_classifier, cat_classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5730289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model libraries\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
    "from sklearn.svm import SVC                          # Support Vector Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier      # Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest\n",
    "from xgboost import XGBClassifier                    # XGBoost\n",
    "from lightgbm import LGBMClassifier                  # LightGBM\n",
    "from catboost import CatBoostClassifier              # CatBoost\n",
    "\n",
    "# Instantiate the models with fixed random states\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "svc_classifier = SVC(kernel=\"linear\", C=1, random_state=42)\n",
    "dt_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=42)\n",
    "xgb_classifier = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "lgb_classifier = LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "cat_classifier = CatBoostClassifier(random_seed=42, verbose=0)\n",
    "\n",
    "# Organize models into families\n",
    "models = {\n",
    "    \"linear models\": [lr_classifier, svc_classifier],\n",
    "    \"tree models\": [dt_classifier, rf_classifier, xgb_classifier, lgb_classifier, cat_classifier]\n",
    "}\n",
    "\n",
    "# Train and save each model\n",
    "for family, model_list in models.items():\n",
    "    for model in model_list:  # Iterate through each model\n",
    "        try:\n",
    "            # Reset the data before training\n",
    "            X_train_tree = X_train_tree_original.copy()\n",
    "            X_train_linear = X_train_linear_original.copy()\n",
    "            X_test_tree = X_test_tree_original.copy()\n",
    "            X_test_linear = X_test_linear_original.copy()\n",
    "            y_train_tree = y_train_tree_original.copy()\n",
    "            y_train_linear = y_train_linear_original.copy()\n",
    "            y_test = y_test_original.copy()\n",
    "\n",
    "            # Fit model depending on family type\n",
    "            if family == \"linear models\":  \n",
    "                model.fit(X_train_linear, y_train_linear)\n",
    "            else:\n",
    "                model.fit(X_train_tree, y_train_tree)\n",
    "            \n",
    "            # Save trained model\n",
    "            model_name = model.__class__.__name__\n",
    "            save_path = f'../saved_models/{model_name}.pkl'\n",
    "            joblib.dump(model, save_path)\n",
    "            print(f\"✅ Saved {model_name} successfully at {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle errors if training or saving fails\n",
    "            print(f\"Failed to train {model.__class__.__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a13a49",
   "metadata": {},
   "source": [
    "## End of modelling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad34383",
   "metadata": {},
   "source": [
    "#### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67521592",
   "metadata": {},
   "source": [
    "## 3. Evaluation: Feature Engineering for Improving Evaluation Metrics\n",
    "\n",
    "Note: Only excecute this section of the code during evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d7052",
   "metadata": {},
   "source": [
    "### 3.1. Service bundle strength\n",
    "\n",
    "Customers with both phone + internet + streaming may behave differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"triple_play\"] = (\n",
    "    (dataset[\"PhoneService\"] == \"Yes\").astype(int) * \n",
    "    (dataset[\"InternetService\"] != \"No\").astype(int) * \n",
    "    ((dataset[\"StreamingTV\"] == \"Yes\") | (dataset[\"StreamingMovies\"] == \"Yes\")).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"triple_play\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c14b16",
   "metadata": {},
   "source": [
    "### 3.2. Security Engagement\n",
    "Combine protection - related services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfe389",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"security_bundle\"] = (\n",
    "    (dataset[\"OnlineSecurity\"] == \"Yes\").astype(int) + \n",
    "    (dataset[\"OnlineBackup\"] == \"Yes\").astype(int) + \n",
    "    (dataset[\"DeviceProtection\"] == \"Yes\").astype(int) +\n",
    "    (dataset[\"TechSupport\"] == \"Yes\").astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"security_bundle\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef3999",
   "metadata": {},
   "source": [
    "### 3.3. Contract × Paperless Billing interaction\n",
    "\n",
    "Some customers with month-to-month + paperless churn faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"contract_paperless\"] = dataset[\"Contract\"].astype(str) + \"_\" + dataset[\"PaperlessBilling\"].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2698535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset[\"contract_paperless\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"contract_paperless\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d72890",
   "metadata": {},
   "source": [
    "### 3.4. High spend per service ratio\n",
    "\n",
    "You already have charge_tenure_ratio, but we can normalize spend by number of services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdfe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"spend_per_service\"] = dataset[\"MonthlyCharges_log\"] / (dataset[\"ServiceCount\"].replace(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset[\"spend_per_service\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c76b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"spend_per_service\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520f860e",
   "metadata": {},
   "source": [
    "### 3.5. Senior + Contract length\n",
    "\n",
    "Older customers may behave differently with long vs. short contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f25ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"senior_contract\"] = dataset[\"SeniorCitizen\"] * dataset[\"contract_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5326549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"senior_contract\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05adfad2",
   "metadata": {},
   "source": [
    "### 3.6. Streaming bundle\n",
    "\n",
    "For internet customers, streaming may be sticky — or indicate high-risk add-ons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa29639",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"streaming_bundle\"] = (\n",
    "    (dataset[\"StreamingTV\"] == \"Yes\").astype(int) +\n",
    "    (dataset[\"StreamingMovies\"] == \"Yes\").astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"streaming_bundle\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daddd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee56180",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfbfa58",
   "metadata": {},
   "source": [
    "#### Note: Go back and continue running again from **section 1.3.12. \"Dropping features that are no longer needed\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdf111d",
   "metadata": {},
   "source": [
    "## End of notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
